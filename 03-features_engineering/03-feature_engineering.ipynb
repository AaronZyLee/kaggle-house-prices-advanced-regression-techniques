{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data fields\n",
    "\n",
    "Here's a brief version of what you'll find in the data description file...\n",
    "\n",
    "* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "* MSSubClass: The building class\n",
    "* MSZoning: The general zoning classification\n",
    "* LotFrontage: Linear feet of street connected to property\n",
    "* LotArea: Lot size in square feet\n",
    "* Street: Type of road access\n",
    "* Alley: Type of alley access\n",
    "* LotShape: General shape of property\n",
    "* LandContour: Flatness of the property\n",
    "* Utilities: Type of utilities available\n",
    "* LotConfig: Lot configuration\n",
    "* LandSlope: Slope of property\n",
    "* Neighborhood: Physical locations within Ames city limits\n",
    "* Condition1: Proximity to main road or railroad\n",
    "* Condition2: Proximity to main road or railroad (if a second is present)\n",
    "* BldgType: Type of dwelling\n",
    "* HouseStyle: Style of dwelling\n",
    "* OverallQual: Overall material and finish quality\n",
    "* OverallCond: Overall condition rating\n",
    "* YearBuilt: Original construction date\n",
    "* YearRemodAdd: Remodel date\n",
    "* RoofStyle: Type of roof\n",
    "* RoofMatl: Roof material\n",
    "* Exterior1st: Exterior covering on house\n",
    "* Exterior2nd: Exterior covering on house (if more than one material)\n",
    "* MasVnrType: Masonry veneer type\n",
    "* MasVnrArea: Masonry veneer area in square feet\n",
    "* ExterQual: Exterior material quality\n",
    "* ExterCond: Present condition of the material on the exterior\n",
    "* Foundation: Type of foundation\n",
    "* BsmtQual: Height of the basement\n",
    "* BsmtCond: General condition of the basement\n",
    "* BsmtExposure: Walkout or garden level basement walls\n",
    "* BsmtFinType1: Quality of basement finished area\n",
    "* BsmtFinSF1: Type 1 finished square feet\n",
    "* BsmtFinType2: Quality of second finished area (if present)\n",
    "* BsmtFinSF2: Type 2 finished square feet\n",
    "* BsmtUnfSF: Unfinished square feet of basement area\n",
    "* TotalBsmtSF: Total square feet of basement area\n",
    "* Heating: Type of heating\n",
    "* HeatingQC: Heating quality and condition\n",
    "* CentralAir: Central air conditioning\n",
    "* Electrical: Electrical system\n",
    "* 1stFlrSF: First Floor square feet\n",
    "* 2ndFlrSF: Second floor square feet\n",
    "* LowQualFinSF: Low quality finished square feet (all floors)\n",
    "* GrLivArea: Above grade (ground) living area square feet\n",
    "* BsmtFullBath: Basement full bathrooms\n",
    "* BsmtHalfBath: Basement half bathrooms\n",
    "* FullBath: Full bathrooms above grade\n",
    "* HalfBath: Half baths above grade\n",
    "* Bedroom: Number of bedrooms above basement level\n",
    "* Kitchen: Number of kitchens\n",
    "* KitchenQual: Kitchen quality\n",
    "* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "* Functional: Home functionality rating\n",
    "* Fireplaces: Number of fireplaces\n",
    "* FireplaceQu: Fireplace quality\n",
    "* GarageType: Garage location\n",
    "* GarageYrBlt: Year garage was built\n",
    "* GarageFinish: Interior finish of the garage\n",
    "* GarageCars: Size of garage in car capacity\n",
    "* GarageArea: Size of garage in square feet\n",
    "* GarageQual: Garage quality\n",
    "* GarageCond: Garage condition\n",
    "* PavedDrive: Paved driveway\n",
    "* WoodDeckSF: Wood deck area in square feet\n",
    "* OpenPorchSF: Open porch area in square feet\n",
    "* EnclosedPorch: Enclosed porch area in square feet\n",
    "* 3SsnPorch: Three season porch area in square feet\n",
    "* ScreenPorch: Screen porch area in square feet\n",
    "* PoolArea: Pool area in square feet\n",
    "* PoolQC: Pool quality\n",
    "* Fence: Fence quality\n",
    "* MiscFeature: Miscellaneous feature not covered in other categories\n",
    "* MiscVal: $Value of miscellaneous feature\n",
    "* MoSold: Month Sold\n",
    "* YrSold: Year Sold\n",
    "* SaleType: Type of sale\n",
    "* SaleCondition: Condition of sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from scipy.stats import norm\n",
    "#from pyglmnet import GLM # Marco: need to understand how to install this \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "from pandas import ExcelWriter  #Marco: this is useful to write in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "#target = train.SalePrice\n",
    "#train.ix[:3].columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distribution and skewness\n",
    "def distskew(dataset,feature):\n",
    "    fig = plt.figure()\n",
    "    sns.distplot(dataset[feature], fit=norm);\n",
    "    return(\"Skewness = \",skew(dataset[feature].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "def scatplot(a,b):\n",
    "    scatplotdata = pd.DataFrame({\"x\":a, \"y\":b})\n",
    "    scatplotdata.plot(x = \"x\", y = \"y\", kind = \"scatter\")\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Marco: uncomment to view this plot\n",
    "#scatplot(train[\"GrLivArea\"],train[\"SalePrice\"])\n",
    "#distskew(train,\"ScreenPorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "train = train.drop(train[train['Id'] == 1299].index)\n",
    "train = train.drop(train[train['Id'] == 524].index)\n",
    "train = train.reset_index()\n",
    "\n",
    "#train = train.drop(train[train['GrLivArea'] > 4000].index)\n",
    "#train = train.drop(train[train['TotalBsmtSF'] == 0].index)\n",
    "\n",
    "#scatplot(train[\"GrLivArea\"],train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "print(\"\\n train\",train.shape)\n",
    "print(\"\\n test\",test.shape)\n",
    "print(\"\\nall_data\",all_data.shape)\n",
    "\n",
    "#get the index of the numerical features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "#Get Categorical data in a fast way:\n",
    "categorical_features = pd.DataFrame(all_data.describe(include = ['O'])).columns\n",
    "\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "#prices.hist()\n",
    "\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Cleaning\n",
    "Add here all the operation useful to remove data with comment that justify your actions (e.g. removing outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#missing data\n",
    "REMOVING_THRESH = 0.8\n",
    "\n",
    "total = all_data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print(missing_data.head(10))\n",
    "#MDV: remove the feature that has more than 10% of missing values\n",
    "print(\"\\n We are going to remove the below features : \\n\",missing_data[missing_data['Percent'] > REMOVING_THRESH])\n",
    "\n",
    "all_data = all_data.drop((missing_data[missing_data['Percent'] > REMOVING_THRESH]).index,1)\n",
    "\n",
    "print(\"\\n After removed data the new dim of all_data are\\n\",all_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some Feature Engineering\n",
    "Put here all the data manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_data[\"TotBsmtFin\"] = all_data[\"BsmtFinSF1\"] + all_data[\"BsmtFinSF2\"]\n",
    "train[\"TotBsmtFin\"] = train[\"BsmtFinSF1\"] + train[\"BsmtFinSF2\"]\n",
    "                 \n",
    "all_data = all_data.drop(\"BsmtFinSF1\",1)\n",
    "all_data = all_data.drop(\"BsmtFinSF2\",1)\n",
    "\n",
    "all_data[\"TotBath\"] = all_data[\"FullBath\"] + 0.5*all_data[\"HalfBath\"] + all_data[\"BsmtFullBath\"] + 0.5*all_data[\"BsmtHalfBath\"]\n",
    "train[\"TotBath\"] = train[\"FullBath\"] + 0.5*train[\"HalfBath\"] + train[\"BsmtFullBath\"] + 0.5*train[\"BsmtHalfBath\"]\n",
    "\n",
    "all_data = all_data.drop(\"FullBath\",1)\n",
    "all_data = all_data.drop(\"HalfBath\",1)\n",
    "all_data = all_data.drop(\"BsmtFullBath\",1)\n",
    "all_data = all_data.drop(\"BsmtHalfBath\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new feature to handle the zero values of TotalBsmtSF\n",
    "#all_data[\"ZeroBsmt\"] = 0\n",
    "#all_data.loc[all_data[\"TotalBsmtSF\"] == 0,\"ZeroBsmt\"] = 1\n",
    "\n",
    "#avgTotalBsmtSF = all_data[\"TotalBsmtSF\"].mean()\n",
    "#all_data.loc[all_data[\"TotalBsmtSF\"] == 0,\"TotalBsmtSF\"] = avgTotalBsmtSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add the living areas and basement aread to create a new feature TotArea\n",
    "all_data[\"TotArea\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n",
    "train[\"TotArea\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take log of the skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the index of the numerical features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "#for all the numeric features, calculate the skewness\n",
    "\n",
    "#skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "\n",
    "#Marco: here i've changed the train[numeric_feats] with all_data[numeric_feats]\n",
    "#because we are working on all_data and train is just a part of it.\n",
    "\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "\n",
    "#exctract the features with skewness higher than 75%\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.1]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "#log transform skewed numeric features with skewness > 75%:\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "train[skewed_feats] = np.log1p(train[skewed_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop some features that do not help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = all_data.drop(\"BsmtFinType1\",1)\n",
    "all_data = all_data.drop(\"2ndFlrSF\",1)\n",
    "all_data = all_data.drop(\"BedroomAbvGr\",1)\n",
    "\n",
    "all_data = all_data.drop(\"LowQualFinSF\",1)\n",
    "all_data = all_data.drop(\"3SsnPorch\",1)\n",
    "#all_data = all_data.drop(\"PoolArea\",1)\n",
    "\n",
    "#all_data = all_data.drop(\"GrLivArea\",1)\n",
    "#all_data = all_data.drop(\"TotalBsmtSF\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(all_data)\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "# Normalize the features (this does not seem to help: increases error)\n",
    "#all_data = all_data.apply(lambda x: x/np.sqrt(sum(x**2)))\n",
    "\n",
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Marco: uncomment to check this plot\n",
    "#scatplot(train[\"GrLivArea\"],train[\"SalePrice\"])\n",
    "#scatplot(train[\"TotalBsmtSF\"],train[\"SalePrice\"])\n",
    "#scatplot(train[\"TotArea\"],train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model: Ridge (L2 Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#first evaluation of the regular linear regression method:\n",
    "#Ridge:\n",
    "model_ridge = Ridge()\n",
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "           for alpha in alphas]\n",
    "    \n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation - Just Do It\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "#Marco: added log scale to have a better view of the minimum\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "print(\"The min value of Ridge is \",cv_ridge.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose the best alpha by taking the alpha that give the lowest rmse\n",
    "best_alpha = 10\n",
    "# Now fit Ridge model\n",
    "model_ridge = Ridge(alpha = best_alpha).fit(X_train, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Fit Model: Lasso (L1 Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distskew(X_train,\"GrLivArea\")\n",
    "#distskew(X_train,\"TotalBsmtSF\")\n",
    "#distskew(X_train,\"TotArea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasso:\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
    "print(\"The min value of Lasso is \",rmse_cv(model_lasso).mean())\n",
    "\n",
    "\n",
    "#Lasso choose performs also feature selection \n",
    "\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "#Let's look to the most important coefficients:\n",
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")  \n",
    "plt.show()  \n",
    "\n",
    "#Also note that unlike the feature importance you'd get from a random forest \n",
    "#these are actual coefficients in your model - so you can say precisely \n",
    "#why the predicted price is what it is. \n",
    "#The only issue here is that we log_transformed both the target \n",
    "#and the numeric features \n",
    "#so the actual magnitudes are a bit hard to interpret.\n",
    "\n",
    "\n",
    "#let's look at the residuals as well:\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "preds_log = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\n",
    "#let's return to \"original\" value from the log:\n",
    "preds_val = np.expm1(preds_log)\n",
    "\n",
    "#solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds_val})\n",
    "#solution.to_csv(\"MDV_Lasso_v1.csv\", index = False)\n",
    "\n",
    "\n",
    "preds_log[\"residuals\"] = preds_log[\"true\"] - preds_log[\"preds\"]\n",
    "\n",
    "\n",
    "\n",
    "#preds_log.plot(x = \"preds\", y = \"true\",kind = \"scatter\")\n",
    "#plt.title('Log Value ')\n",
    "#plt.show()\n",
    "\n",
    "#\n",
    "#preds_val[\"residuals\"] = preds_val[\"true\"] - preds_val[\"preds\"]\n",
    "#preds_val.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\n",
    "#plt.title('Original Value in $')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's add an xgboost model to our linear model to see \n",
    "#if we can improve our score:\n",
    "\n",
    "#y = preds_log[\"residuals\"]\n",
    "    \n",
    "dtrain = xgb.DMatrix(X_train, label = y)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"max_depth\":2, \"eta\":0.1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n",
    "\n",
    "model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n",
    "plt.show()\n",
    "\n",
    "#the params were tuned using xgb.cv\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=300, max_depth=2, learning_rate=0.1) \n",
    "model_xgb.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now fit GLM net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_glm = GLM(distr='gaussian')\n",
    "#scaler1 = StandardScaler().fit(X_train)\n",
    "#scaler2 = StandardScaler().fit(X_test)\n",
    "#model_glm.fit(scaler1.transform(X_train), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now combine the models for final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "ridge_preds = np.expm1(model_ridge.predict(X_test))\n",
    "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
    "\n",
    "#Marco: I still don't have the glm available on my Jupyter env\n",
    "#glm_preds = np.expm1(model_glm.predict(scaler2.transform(X_test)))\n",
    "predictions_lasso = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\n",
    "#predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")\n",
    "#plt.show()\n",
    "\n",
    "predictions_ridge = pd.DataFrame({\"xgb\":xgb_preds, \"ridge\":ridge_preds})\n",
    "\n",
    "lasso_vs_ridge = pd.DataFrame({\"lasso\":lasso_preds, \"ridge\":ridge_preds})\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "fig.suptitle(\"Prediction check\", fontsize=16)\n",
    "ax = plt.subplot(\"131\")\n",
    "ax.set_title(\"XGB VS Lasso\")\n",
    "ax.scatter(xgb_preds,lasso_preds)\n",
    "\n",
    "#axes = plt.gca()\n",
    "#ax.set_xlim([xmin,xmax])\n",
    "#axes.set_ylim([ymin,ymax])\n",
    "\n",
    "ax = plt.subplot(\"132\")\n",
    "ax.set_title(\"XGB VS Ridge\")\n",
    "ax.scatter(xgb_preds,ridge_preds)\n",
    "ax.autoscale(tight=True)\n",
    "\n",
    "ax = plt.subplot(\"133\")\n",
    "ax.set_title(\"Lasso VS Ridge\")\n",
    "ax.scatter(lasso_preds,ridge_preds,alpha=.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#predictions_ridge = pd.DataFrame({\"xgb\":xgb_preds, \"ridge\":ridge_preds})\n",
    "#predictions.plot(x = \"xgb\", y = \"ridge\", kind = \"scatter\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Many times it makes sense to take a weighted average of uncorrelated results - this usually imporoves \n",
    "#the score although in this case it doesn't help that much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#preds = 0.30*lasso_preds + 0.30*ridge_preds + 0.40*xgb_preds\n",
    "\n",
    "#Marco: for now we can frozen this strategy and go ahead with feature engineering\n",
    "preds = 0.60*lasso_preds + 0.40*xgb_preds\n",
    "\n",
    "solution = pd.DataFrame({\"SalePrice\":preds,\"id\":test.Id})\n",
    "\n",
    "#N.B. USE YOUR RELATIVE PATH HERE FOR THE CORRECT OUTPUT:\n",
    "#Obaidur:\n",
    "solution.to_csv(\"D:/acads/ds/kaggle/house-price/my_solution_eight.csv\", index = False)\n",
    "#Marco\n",
    "#solution.to_csv(\"C:\\\\Users\\Marco Di Vivo\\\\Documents\\\\Marco\\\\MachineLearning\\\\0_Kaggle\\\\1_HousePrediction\\\\Lasso_160417.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Marco : with the above code we get 0.11965 score on the LB   16/04/2017\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
